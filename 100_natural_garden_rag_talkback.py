# -*- coding: utf-8 -*-
"""
RAG èªéŸ³å°ç­”æ©Ÿå™¨äººï¼šéŒ„éŸ³(VAD) -> ASR -> RAGæª¢ç´¢ -> ç”Ÿæˆå›è¦† -> TTSæ’­æ”¾

ä¾è³´ï¼š
- pyaudio (éŒ„éŸ³)
- aplay (æ’­æ”¾ wavï¼›æˆ–è‡ªè¡Œæ”¹ç‚º afplay/ffplay)
- langchain, langchain-openai, langchain-community
- openai
- pandas (CSV loader æœƒç”¨åˆ°)
- python-dotenv (å¯é¸ï¼Œç”¨æ–¼è¼‰å…¥ .env)

ç’°å¢ƒè®Šæ•¸ï¼š
- OPENAI_API_KEY

è³‡æ–™ï¼š
- ä¸€å€‹ CSVï¼Œä¾‹å¦‚ NaturalGarden.csvï¼ˆå«ä½ è¦æª¢ç´¢çš„å…§å®¹ï¼‰
"""

import os
import time
import wave
import math
import audioop
import shutil
import subprocess
import collections
from typing import List, TypedDict, Optional

import pyaudio
from openai import OpenAI

# ===== å¯èª¿åƒæ•¸ =====
SAMPLE_RATE = 16000
CHANNELS = 1
SAMPLE_WIDTH = 2               # 16-bit
FRAME_DURATION_MS = 30
CALIBRATE_SEC = 0.5
SILENCE_TIMEOUT_SEC = 1.8
MAX_UTTERANCE_SEC = 60
THRESHOLD_BOOST = 1.6

TEMP_WAV = "/tmp/asr_input.wav"
TTS_WAV = "/tmp/tts_output.wav"

ASR_MODEL = "gpt-4o-mini-transcribe"   # ä½ åŸæœ¬ç”¨çš„
TTS_MODEL = "gpt-4o-mini-tts"
TTS_VOICE = "nova"

# RAG / LLM è¨­å®š
EMBED_MODEL = "text-embedding-3-large"
GEN_MODEL = "gpt-4.1-mini"  # æ–‡å­—å›ç­”çš„æ¨¡å‹ï¼Œå¯æ› "gpt-4o-mini" ç­‰
CSV_PATH = "NaturalGarden.csv"
TOP_K = 4  # å¬å›æ–‡ä»¶æ•¸

# ====== LangChain / RAG ======
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.vectorstores import InMemoryVectorStore

# -- RAG æç¤ºæ¨¡æ¿ï¼ˆé¿å…ä¾è³´ hub.pullï¼‰--
RAG_PROMPT = """You are a helpful assistant. Use ONLY the context to answer the question.
If the answer is not in the context, say you don't know.
Answer in the same language as the user's question.

Question:
{question}

Context:
{context}
"""

def ensure_cmd_exists(cmd):
    return shutil.which(cmd) is not None

def play_wav_with_aplay(path):
    # ä½ ä¹Ÿå¯æ”¹æˆ 'afplay' (macOS) æˆ– 'ffplay -autoexit -nodisp'
    player = "aplay"
    if not ensure_cmd_exists(player):
        print(f"Cannot play audio: '{player}' command not found.")
        return
    try:
        subprocess.run([player, path], check=False)
    except Exception as e:
        print(f"[Play] exception: {e}")

def frame_bytes():
    samples = int(SAMPLE_RATE * (FRAME_DURATION_MS / 1000.0))
    return samples * CHANNELS * SAMPLE_WIDTH

def write_wav(path, audio_bytes):
    with wave.open(path, "wb") as wf:
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(SAMPLE_WIDTH)
        wf.setframerate(SAMPLE_RATE)
        wf.writeframes(audio_bytes)

def rms(data):
    return audioop.rms(data, SAMPLE_WIDTH)

def record_until_silence():
    pa = pyaudio.PyAudio()
    stream = pa.open(format=pyaudio.paInt16,
                     channels=CHANNELS,
                     rate=SAMPLE_RATE,
                     input=True,
                     frames_per_buffer=int(SAMPLE_RATE * FRAME_DURATION_MS / 1000),
                     input_device_index=None)

    fb = frame_bytes()
    voiced = bytearray()
    start_time = time.time()
    last_loud_time = None

    # 1) Auto calibrate
    noise_samples = []
    need_frames = int(CALIBRATE_SEC * 1000 / FRAME_DURATION_MS)
    print(f"\nCalibrating noise for {CALIBRATE_SEC} sec...")
    for _ in range(need_frames):
        chunk = stream.read(int(SAMPLE_RATE * FRAME_DURATION_MS / 1000), exception_on_overflow=False)
        if len(chunk) == fb:
            noise_samples.append(rms(chunk))
    noise_floor = (sum(noise_samples) / max(1, len(noise_samples))) if noise_samples else 0
    threshold = max(200, noise_floor * THRESHOLD_BOOST)
    print(f"Noise floor RMS â‰ˆ {noise_floor:.1f}. Threshold â‰ˆ {threshold:.1f}")
    print("Start talking (Ctrl+C to Quit)")

    prebuf = collections.deque(maxlen=int(300/FRAME_DURATION_MS))  # 300ms pre-roll
    speech_started = False

    try:
        while True:
            data = stream.read(int(SAMPLE_RATE * FRAME_DURATION_MS / 1000), exception_on_overflow=False)
            if len(data) != fb:
                continue

            level = rms(data)
            is_loud = level >= threshold

            if not speech_started:
                prebuf.append(data)
                if is_loud:
                    speech_started = True
                    last_loud_time = time.time()
                    for b in prebuf:
                        voiced.extend(b)
            else:
                voiced.extend(data)
                if is_loud:
                    last_loud_time = time.time()

                if last_loud_time and (time.time() - last_loud_time) >= SILENCE_TIMEOUT_SEC:
                    break

            if (time.time() - start_time) > MAX_UTTERANCE_SEC:
                print("Speak time limit reached.")
                break

    except KeyboardInterrupt:
        print("\nRecording stopped by user.")
    finally:
        stream.stop_stream()
        stream.close()
        pa.terminate()

    return bytes(voiced)

# ===== RAG Pipeline æ§‹å»º =====
class RAGBot:
    def __init__(self, csv_path: str, embed_model: str, gen_model: str, top_k: int = 4):
        self.csv_path = csv_path
        self.top_k = top_k

        # Embeddings + å‘é‡åº«
        self.embeddings = OpenAIEmbeddings(model=embed_model)
        self.vector_store = InMemoryVectorStore(self.embeddings)

        # è¼‰å…¥ CSV -> åˆ‡å¡Š -> å»ºç´¢å¼•
        self._load_and_index_csv(csv_path)

        # æ–‡å­—ç”Ÿæˆæ¨¡å‹ï¼ˆèµ° LangChain ChatOpenAIï¼Œæ–¹ä¾¿å¾ŒçºŒæ›æˆå·¥å…·èª¿ç”¨ç­‰ï¼‰
        self.llm = ChatOpenAI(model=gen_model, temperature=0.2)

    def _load_and_index_csv(self, path: str):
        if not os.path.exists(path):
            raise FileNotFoundError(f"CSV not found: {path}")
        loader = CSVLoader(path)
        docs = loader.load()

        # ä½ ä¹Ÿå¯æŠŠæ¯åˆ—æŸæ¬„ä½æŒ‘å‡ºä¾†çµ„åˆï¼Œé€™è£¡å…ˆç°¡åŒ–æ•´åˆ—æ–‡å­—
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        chunks = splitter.split_documents(docs)

        if not chunks:
            raise ValueError("No chunks produced from CSV. Check file content/encoding.")
        _ = self.vector_store.add_documents(chunks)
        print(f"Indexed {len(chunks)} chunks from {path}")

    def _format_context(self, docs: List[Document]) -> str:
        parts = []
        for i, d in enumerate(docs, 1):
            meta = d.metadata or {}
            parts.append(f"[{i}] " + d.page_content)
        return "\n\n".join(parts)

    def answer(self, question: str) -> str:
        retrieved = self.vector_store.similarity_search(question, k=self.top_k)
        ctx = self._format_context(retrieved)
        prompt = RAG_PROMPT.format(question=question, context=ctx)
        # LangChain çš„ ChatOpenAI æ¥å£ï¼šè¼¸å…¥ä¸€å€‹ human message å­—ä¸²å³å¯
        resp = self.llm.invoke(prompt)
        text = resp.content.strip() if hasattr(resp, "content") else str(resp)
        return text

# ===== ä¸»ç¨‹å¼ï¼šASR -> RAG -> TTS =====
def main():
    # è¼‰ .envï¼ˆå¯é¸ï¼‰
    try:
        from dotenv import load_dotenv
        load_dotenv()
    except Exception:
        pass

    # Init OpenAI client
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise RuntimeError("OPENAI_API_KEY is not set.")
    client = OpenAI(api_key=api_key)

    # æº–å‚™ RAG Bot
    print("Building RAG index...")
    rag = RAGBot(csv_path=CSV_PATH, embed_model=EMBED_MODEL, gen_model=GEN_MODEL, top_k=TOP_K)

    print("\nRAG Voice Bot is running.")
    print("Speak after the 'Start talking' line; I'll answer from CSV knowledge and talk back.\n")

    try:
        while True:
            # 1) éŒ„éŸ³ï¼ˆå« VADï¼‰
            audio_bytes = record_until_silence()
            if not audio_bytes:
                print("No audio recorded. Listening again...")
                continue

            write_wav(TEMP_WAV, audio_bytes)
            print(f"Saved input to {TEMP_WAV}")

            # 2) ASRï¼šèªéŸ³è½‰æ–‡å­—
            print("Transcribing...")
            try:
                with open(TEMP_WAV, "rb") as f:
                    transcript = client.audio.transcriptions.create(
                        model=ASR_MODEL,
                        file=f
                    )
                text = (transcript.text or "").strip()
                print("You said:", text if text else "(empty)")
            except Exception as e:
                print(f"[ASR] Exception: {e}")
                continue

            if not text:
                print("Nothing recognized. Listening again...")
                continue

            # 3) RAG ç”¢ç”Ÿå›ç­”
            try:
                answer = rag.answer(text)
            except Exception as e:
                print(f"[RAG] Exception: {e}")
                answer = "æŠ±æ­‰ï¼Œæˆ‘å‰›å‰›æª¢ç´¢è³‡æ–™æ™‚é‡åˆ°å•é¡Œã€‚"

            print("Bot:", answer)

            # 4) TTSï¼šæŠŠå›ç­”è¬›å‡ºä¾†
            print("Synthesizing speech...")
            try:
                with client.audio.speech.with_streaming_response.create(
                    model=TTS_MODEL,
                    voice=TTS_VOICE,
                    input=answer,
                    response_format="wav",
                ) as response:
                    response.stream_to_file(TTS_WAV)
                print(f"Generated {TTS_WAV}")
                play_wav_with_aplay(TTS_WAV)
            except Exception as e:
                print(f"[TTS] Exception: {e}")

            print("Listening again... (Ctrl+C to Quit)")

    except KeyboardInterrupt:
        print("\nBye! ğŸ‘‹")

if __name__ == "__main__":
    main()
